---
title: "cars Project"
author: "George"
date: "June 19, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(dplyr)
library(tidyr)
library(DataExplorer)
library(broom)
library(janitor)
library(car)
library(recipes)

data <- read.csv("Cars.csv", header = T)

data <- data%>%janitor::clean_names()

#####create a new column called car which shows whether a person has a car or not#####

data <- data%>%mutate(Car = case_when (
                              transport %in% c("2Wheeler", "Public Transport") ~ "No Car",
                              transport %in% c("Car") ~ "Car"
          )
)%>%mutate( Car = as.factor(Car))


###create data exploration report#####

create_report(
                data = data, 
                output_file = "Car_case.html",
                output_dir = getwd(), 
                config = configure_report( add_plot_bar = TRUE, 
                                           add_plot_intro = TRUE, 
                                           add_plot_density = TRUE, 
                                           add_plot_prcomp = TRUE, 
                                           add_plot_correlation = TRUE, 
                                           add_plot_scatterplot = FALSE, 
                                           add_plot_qq = TRUE
                  
                  
                )
  
  
)

##############licence, engineer, and mba are factors########
plot_boxplot(data%>%select(-c(engineer, mba,license)), by = "transport")
plot_boxplot(data%>%select(-c(engineer, mba,license)), by = "Car")
#############Plot bar plot with colors###########################



########the mba has empty values, train the bagging imputation on the non-empty values 


test_impute <- data%>%filter(is.na(mba == T))



train_impute <- data%>%filter(is.na(mba) == F)




#####make the recipe to impute the values, the experience and the number of family members



recipe_data <-  recipe(mba ~., data = train_impute)



impute_data <- recipe_data%>%
                    step_impute_bag( mba,
                                   seed_val = 123,
                                   trees = 30)


run_impute <- prep(impute_data, training = train_impute)


impute <- bake(run_impute, new_data = test_impute, everything())
   

data_new <- train_impute%>%bind_rows(impute)






```


```{r}

library(ggplot2)
library(ggthemes)
plot_cats <- data_new%>%mutate(transport = as.factor(transport),
                               engineer = as.factor(engineer), 
                               mba = as.factor(mba),
                               license = as.factor(license),
                               gender = as.factor(gender))%>%
                       select_if(is.factor)%>%
                       #select(-c(id))%>%
                       select(transport, everything())%>%
                       tidyr::pivot_longer(cols = gender:license , 
                        names_to = "Variables",
                        values_to = "Value")


try <- plot_cats%>%#partition(cluster)%>%
                  count(transport, Variables,Value )%>%
                  collect()

#########################plotting the densities#####################


plot_cont <- data_new%>%select(transport, age, salary,work_exp, distance )%>%
             select(transport, everything())%>%
            ###adding group_by to not drop personal_loan###
             group_by(transport)%>%
                  #select_if(is.double)%>%
                  #select(-family_members)%>%
             pivot_longer(cols = age:distance, 
                         names_to = "Variables", values_to = "Value")
                    
#####create rows samples with 1:8



pdf("variation_with_loan.pdf", paper = "special", width = 14, height = 8)
plot_cont%>%ggplot(aes(x = Value, fill = transport , color = transport))+
            facet_wrap(~Variables , scales = "free", ncol = 5)+
            geom_density(alpha = 0.5, adjust = 2)+
            scale_color_tableau()+
            scale_fill_tableau()+
            theme_minimal()
dev.off()

##########################################


pdf("categorical_1.pdf", paper = "special", width = 14, height = 8)
try%>%ggplot(aes(x = Value, y = n , color = transport, fill = transport))+
            facet_wrap(~ Variables, scales = "free", ncol = 3)+
            geom_bar(stat = "identity")+
            scale_color_tableau()+
            scale_fill_tableau()+
            theme_minimal()
dev.off()
######
library(ggplot2)
library(ggthemes)
```


```{r}


library(ggplot2)
library(ggthemes)
plot_cats <- data_new%>%mutate(transport = as.factor(transport),
                               engineer = as.factor(engineer), 
                               mba = as.factor(mba),
                               license = as.factor(license),
                               gender = as.factor(gender))%>%
                       select_if(is.factor)%>%
                       #select(-c(id))%>%
                       select(Car, everything())%>%
                       tidyr::pivot_longer(cols = gender:transport , 
                        names_to = "Variables",
                        values_to = "Value")


try <- plot_cats%>%#partition(cluster)%>%
                  count(Car, Variables,Value )%>%
                  collect()

#########################plotting the densities#####################


plot_cont <- data_new%>%select(Car, age, salary,work_exp, distance )%>%
             select(Car, everything())%>%
            ###adding group_by to not drop personal_loan###
             group_by(Car)%>%
                  #select_if(is.double)%>%
                  #select(-family_members)%>%
             pivot_longer(cols = age:distance, 
                         names_to = "Variables", values_to = "Value")
                    
#####create rows samples with 1:8



pdf("variation_with_loan_car.pdf", paper = "special", width = 14, height = 8)
plot_cont%>%ggplot(aes(x = Value, fill = Car , color = Car))+
            facet_wrap(~Variables , scales = "free", ncol = 5)+
            geom_density(alpha = 0.5, adjust = 2)+
            scale_color_tableau()+
            scale_fill_tableau()+
            theme_minimal()
dev.off()

##########################################


pdf("categorical__car_1.pdf", paper = "special", width = 14, height = 8)
try%>%ggplot(aes(x = Value, y = n , color = Car, fill = Car))+
            facet_wrap(~ Variables, scales = "free", ncol = 3)+
            geom_bar(stat = "identity")+
            scale_color_tableau()+
            scale_fill_tableau()+
            theme_minimal()
dev.off()
######
library(ggplot2)
library(ggthemes)

data



```


#####Prepare the data###

```{r}






library(GGally)

pdf("ggpairs_data.pdf", paper = "special", width = 14, height = 8)
ggpairs(data%>%select(-transport), aes( color = as.factor(Car) )) + theme_bw()

dev.off()


## Import libraries
library(FactoMineR)
library(factoextra)
set.seed(123)

res.famd <- FAMD(data_new%>%dplyr::select(-transport)%>%
                        mutate(engineer = as.factor(engineer),
                               mba = as.factor(mba),
                               license = as.factor(license)), 
                 sup.var = 9,  ## Set the target variable "Churn" as a supplementary variable, so it is not included in the analysis for now
                 graph = FALSE, 
                 ncp=25)


pca_data <- get_famd_ind(res.famd)$coord


######correlation plot for variables, we choose 6 dimensions
corrplot::corrplot(get_famd_var(res.famd)$cos2, is.corr=FALSE)

data_use <- cbind.data.frame(pca_data, data%>%dplyr::select(Car))%>%
                            dplyr::select(Car, everything())%>%
                            clean_names()





```



```{r}

library(caret)
library(minDiff)
#set.seed(123)



#########sample the minimal class and the maximal class as well.



## 8.373206% is the minimal set and the maximal set is 91.62679


data_use_knn <- data_use%>%mutate(car = as.factor(if_else(car == "Car", 1,0)))%>%
                          mutate(ID = 1:nrow(data_use))

 


#####################################################

#set.seed(123)
set.seed(400)

train_no_car  <- data_use_knn%>%filter(car == 0)%>%
                                  group_by(car)%>%
                                  sample_n(267)


train_car <- data_use_knn%>%filter(car == 1)%>%
                        group_by(car)%>%
                        sample_n(24)




test_no_car <- data_use_knn %>%filter(car == 0)%>%
                              group_by(car)%>%
                              anti_join(train_no_car)



test_car <-   data_use_knn %>%filter(car == 1)%>%
                               group_by(car)%>%
                               anti_join(train_car)


train_set <- rbind.data.frame(train_car, train_no_car)
#####################randomise the order###############################
train_set <- train_set[sample(nrow(train_set)), 1:7]


test_set <- rbind.data.frame(test_car, test_no_car)
#####################randomise the order###############################
test_set <- test_set[sample(nrow(test_set)), 1:7]










#install.packages("brglm2")
```




####LOGISTIC REGRESSSION##########

```{r}
library("brglm2")
library(MASS)
#---------------------------------------------------------    


#logisitic_train <- train_set%>%mutate_if(is.double, scale)
### Modelling: Logistic Regression
logistic_1 <- glm(car ~ ., family = binomial, data = train_set)

summary(logistic_1)




logistic_2 <- stepAIC(logistic_1, direction = "both",k=5)

vif(logistic_2)

logistic_3 <- glm(car ~ dim_1 + dim_2 + dim_3 +  dim_6, family = binomial(link = probit), data = train_set)
vif(logistic_3)
summary(logistic_3)


######VIF AIC tradeoff
logistic_4 <- glm(car ~ dim_1 + dim_2 + dim_5 +  dim_6, family = binomial(link = probit), data = train_set)
vif(logistic_4)

summary(logistic_4)

#####################################

knn_train <- train_set%>%dplyr::select(car, dim_1, dim_2,dim_3,dim_4, dim_5,dim_6 )%>%mutate(Label = as.factor(if_else( car== "1", "Car", "No_Car")))%>%
                         ungroup()%>%
                         dplyr::select(-car)%>%
                         mutate(Label = as.factor(as.character(Label)))

knn_test <- test_set%>%dplyr::select(car, dim_1, dim_2,dim_3,dim_4, dim_5,dim_6 )%>%mutate(Label = as.factor(if_else( car== "1", "Car", "No_Car")))%>%
                         ungroup()%>%
                         dplyr::select(-car)%>%
                         mutate(Label = as.factor(as.character(Label)))



ctrl <- trainControl(classProbs = TRUE, 
                     summaryFunction = twoClassSummary,
                    savePredictions = TRUE,
                     allowParallel = TRUE)

mod_fit <- caret::train(Label ~ dim_1 + dim_2 + dim_5 +  dim_6,  data=knn_train, method="glm", family="binomial", trControl = ctrl)

pred_logistic <- predict(mod_fit, newdata=knn_test, type = "prob")
pred_response <- predict(mod_fit, newdata=knn_test, type = "raw")


confusionMatrix( as.factor( pred_response),as.factor(knn_test$Label ))


resample_stats_logisitic <- thresholder(mod_fit, 
                              threshold = seq(.00, 1, by = 0.02), 
                              final = TRUE)

resample_stats_logisitic%>%arrange(desc(Sensitivity))


####choose threshold of 
logistic_predictions <- pred_logistic%>%mutate(final_class_0.28 = if_else(Car < 0.28, "No_Car", "Car"),
                                       final_class_0.06 = if_else(Car < 0.06, "No_Car", "Car"))



confusionMatrix( as.factor( logistic_predictions$final_class_0.28),as.factor(knn_test$Label ))


confusionMatrix( as.factor( logistic_predictions$final_class_0.06),as.factor(knn_test$Label ))



```



```{r}
train_set
```





```{r}
library(ROCR)
library(grid)
library(broom)
library(caret)
library(tidyr)
library(dplyr)
library(scales)
library(ggplot2)
library(brglm)
library(ggthemes)
library(gridExtra)
library(data.table)


mn <- brglm( car~ ., family=binomial, data=train_set)


#mn <- brglm( car~ ., family=binomial, data=train_set%>%dplyr::select(-c(pred, real)))

summary(mn)


f <- stepAIC(logistic_1, direction = "both",k=5)
summary(f)




plot(varImp(mod_fit, scale = T))
```


```{r}
library(lattice)
library(rpart)
library(brglm)
#install.packages("brglm")




xyplot(train_set$dim_1 ~ train_set$dim_3 , group = train_set$car, grid = T, asp = 1)
xyplot(train_set$dim_1 ~ train_set$dim_5 , group = train_set$car, grid = T, asp = 1)



xyplot(data_new$salary ~ data_new$distance , group = data_new$Car, grid = T, asp = 1)
xyplot(data_new$age ~ data_new$distance , group = data_new$Car, grid = T, asp = 1)
xyplot(data_new$work_exp ~ data_new$distance , group = data_new$Car, grid = T, asp = 1)

data_new


xyplot(train_set$car ~ train_set$dim_1 , group = train_set$car, grid = T, asp = 1)
xyplot(train_set$Car ~ train_set$Dim.3 , group = train_set$Car, grid = T, asp = 1)
xyplot(train_set$Car ~ train_set$Dim.4 , group = train_set$Car, grid = T, asp = 1)
xyplot(train_set$Car ~ train_set$Dim.5 , group = train_set$Car, grid = T, asp = 1)
xyplot(train_set$Car ~ train_set$Dim.6 , group = train_set$Car, grid = T, asp = 1)



```





#####KNN USING CARET#################



```{r}
library(caret)

knn_train <- train_set%>%dplyr::select(car, dim_1, dim_2,dim_3,dim_4, dim_5,dim_6 )%>%mutate(Label = as.factor(if_else( car== "1", "Car", "No_Car")))%>%
                         ungroup()%>%
                         dplyr::select(-car)%>%
                         mutate(Label = as.factor(as.character(Label)))

knn_test <- test_set%>%dplyr::select(car, dim_1, dim_2,dim_3,dim_4, dim_5,dim_6 )%>%mutate(Label = as.factor(if_else( car== "1", "Car", "No_Car")))%>%
                         ungroup()%>%
                         dplyr::select(-car)%>%
                         mutate(Label = as.factor(as.character(Label)))

#preProcValues <- preProcess(x = knn_train,method = c("center", "scale"))


set.seed(400)
ctrl <- trainControl(method="repeatedcv",repeats = 10 ,classProbs=TRUE, summaryFunction = twoClassSummary, savePredictions=TRUE)


knnFit <- caret::train(Label ~ ., data = knn_train, method = "knn", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 20)


#Output of kNN fit
knnFit

```




```{r}


plot(knnFit)

```


```{r}


knnPredict <- predict(knnFit,newdata = knn_test , type = "prob")
knnPredict_values <- predict(knnFit,newdata = knn_test )
#Get the confusion matrix to see accuracy value and other parameter values
confusionMatrix(knnPredict_values, as.factor(knn_test$Label ))


resample_stats <- thresholder(knnFit, 
                              threshold = seq(.00, 1, by = 0.02), 
                              final = TRUE)

resample_stats%>%arrange(desc(Sensitivity))


####choose threshold of 
knn_predictions <- knnPredict%>%mutate(final_class_0.1 = if_else(Car < 0.1, "No_Car", "Car"),
                                       final_class_0.06 = if_else(Car < 0.06, "No_Car", "Car"))



confusionMatrix( as.factor( knn_predictions$final_class_0.1),as.factor(knn_test$Label ))

  

```



```{r}
set.seed(400)
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0:5,
  adjust = seq(0, 5, by = 1)
)


train_control <- trainControl(
  method="repeatedcv", 
  repeats = 10 ,
  classProbs=TRUE, 
  summaryFunction = twoClassSummary, 
  savePredictions=TRUE
  )

x_1 = setdiff(colnames(knn_train), c("Label"))

x <- as.data.frame(knn_train[,x_1])
y <- knn_train$Label

nb.m1 <- caret::train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control,
  tuneGrid = search_grid,
  preProc = c("BoxCox", "center", "scale")
  )


nb.m1$results %>% 
  top_n(5, wt = ROC) %>%
  arrange(desc(ROC))


NBpred <- predict(nb.m1, newdata = knn_test, type = "prob")
NBresponse <- predict(nb.m1, newdata = knn_test)
confusionMatrix(as.factor(NBresponse), as.factor(knn_test$Label))

#################

resample_stats <- thresholder(nb.m1, 
                              threshold = seq(.00, 1, by = 0.02), 
                              final = TRUE)

resample_stats%>%arrange(desc(Sensitivity))


####choose threshold of 
NB_predictions <- NBpred%>%mutate(final_class_0.02 = if_else(Car < 0.02, "No_Car", "Car"),
                                       final_class_0.20 = if_else(Car < 0.20, "No_Car", "Car"),
                                       final_class_0.48 = if_else(Car < 0.48, "No_Car", "Car"))


confusionMatrix( as.factor( NB_predictions$final_class_0.48),as.factor(knn_test$Label ))

plot(varImp(nb.m1, scale = T))


```






###########################


```{r}

library(digest)
library(mlr)         # Machine learning library
library(parallelMap)

set.seed(400)

ml_task <- makeClassifTask(data = knn_train,
                            target = "Label",  positive = "Car")

# Create repeated cross validation folds
#cv_folds <- makeResampleDesc("RepCV",reps=2,folds=3) # repeated CV
cv_folds <- makeResampleDesc("CV", iters = 5) # 3 fold cross validation

# Define model tuning algorithm ~ Random tune algorithm
random_tune <- makeTuneControlRandom(maxit = 8)  # 1 iteration for illustration purposes

# Define model
model <- makeLearner("classif.xgboost", predict.type = "prob") # Regression XgBoost model

# Define parameters of model and search grid ~ !!!! MODEL SPECIFIC !!!!
model_Params <- makeParamSet(
  makeIntegerParam("nrounds",lower=10,upper=100),
  makeIntegerParam("max_depth",lower=1,upper=20),
  makeNumericParam("lambda",lower=0.55,upper=0.60),
  makeNumericParam("eta", lower = 0.001, upper = 1),
  #makeNumericParam("subsample", lower = 0.10, upper = 0.80),
  #makeNumericParam("min_child_weight",lower=1,upper=5),
  makeNumericParam("colsample_bytree",lower = 0.2,upper = 0.8)
)


parallelStartSocket(8)

tuned_model <- tuneParams(learner = model,
                        task = ml_task,
                        resampling = cv_folds,
                        measures = tpr,       # R-Squared performance measure, this can be changed to one or many
                        par.set = model_Params,
                        control = random_tune,
                        show.info = FALSE)



model <- setHyperPars(learner = model,
                        par.vals = tuned_model$x)



# Verify performance on cross validation folds of tuned model
resample(model,ml_task,cv_folds,measures = list(acc,tpr, f1, tnr))


# Train final model with tuned parameters
xgBoost <- train(learner = model,task = ml_task)



# Predict on test set
preds <- predict(xgBoost, newdata = knn_test)

# Stop parallel instance ~ Good practice to retire cores when training is complete
confusionMatrix(as.factor(preds$data$response), as.factor(knn_test$Label))

pred = setThreshold(preds, c(Car= 1, No_Car = 1))
(calculateConfusionMatrix(pred))

importance <- getFeatureImportance(xgBoost)



importance$res %>%
  arrange(desc(importance)) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
  mutate(name=factor(variable, levels=variable)) %>%   # This trick update the factor levels
  ggplot( aes(x=reorder(variable, importance), y=importance)) +
    geom_segment( aes(xend=variable, yend=0)) +
    geom_point( size=4, color="blue") +
    coord_flip() +
    theme_bw() +
    xlab("")




parallelStop()

#str(preds)



  



```





```{r}

library(caret)
library(randomForest)
library(mlr)
library(parallelMap)

parallelStartSocket(8)
set.seed(400)



train.task <- makeClassifTask(data = knn_train , 
                        target = "Label", 
                        id = "mlr_1",
                        positive = "Car")

###Create the learner###########

learner = makeLearner("classif.randomForest", predict.type = "prob", importance = TRUE)

getParamSet(learner)

#####create the parameter set in the data#3


resampling.strategy <-   makeResampleDesc("CV", iters = 10)
parameter_set <- makeParamSet(makeIntegerParam("ntree", lower = 10, upper = 500),
                              makeIntegerParam("mtry", lower = 3, upper = 24))


tune.ctrl = makeTuneControlRandom(maxit = 8)

#########
tune_learner <- tuneParams(learner = learner, 
                           task = train.task,
                           resampling = resampling.strategy,
                           par.set = parameter_set,
                           control = makeTuneControlGrid())

parallelStop()


tuned_learner <- setHyperPars(makeLearner("classif.randomForest", importance = TRUE, predict.type = "prob"),
                              par.vals = tune_learner$x)

# Verify performance on cross validation folds of tuned model
#resample(tuned_learner,train.task,resampling.strategy,measures = list(acc,tpr, f1, tp))

# apply optimal hyperparameter set



train.res <- train(tuned_learner,
                   train.task
                   )

# print final results
print(train.res)

# predict on test data

task.test <- makeClassifTask(id     = "mlr_test",
                             data   = knn_test,
                             target = "Label")


predict.res_thresh <- predict(train.res, task.test)
predict.res <- predict(train.res, task.test)$data$response


######################

confusionMatrix(predict.res, knn_test$Label)



pred_rand = setThreshold(predict.res_thresh, c(Car = 1, No_Car = 1))
(calculateConfusionMatrix(pred_rand))


pred$threshold


importance <- getFeatureImportance(train.res)

importance$res %>%
  arrange(desc(importance)) %>%    # First sort by val. This sort the dataframe but NOT the factor levels
  mutate(name=factor(variable, levels=variable)) %>%   # This trick update the factor levels
  ggplot( aes(x=reorder(variable, importance), y=importance)) +
    geom_segment( aes(xend=variable, yend=0)) +
    geom_point( size=4, color="blue") +
    coord_flip() +
    theme_bw() +
    xlab("")

```
###removing the oultiers from the data#####

```{r}

# Outlier Treatment #
#####################

# Two Ways: 
# 1. Box Plot Method: Flooring: Q1-1.5IQR(Interquartile Range) and Capping with Q3+1.5IQR
# 2. Percentile Distribution: Flooring : 1% and Capping at 99% (General Industry Practice - If you don't have a business understanding)


#In this exercise, as we don't have enough info about data understanding
# we will follow 2nd way to treat outliers in one go

# Let's define a function:
outlier_treatment_fun = function(data,var_name){
  capping = as.vector(quantile(data[,var_name],0.99))
  flooring = as.vector(quantile(data[,var_name],0.01))
  data[,var_name][which(data[,var_name]<flooring)]<- flooring
  data[,var_name][which(data[,var_name]>capping)]<- capping
  #print('done',var_name)
  return(data)
}


data_outlier <- data_new%>%dplyr::select(age,distance, salary, work_exp, gender, engineer, mba, license, Car)

vars <- c("age","distance", "salary", "work_exp")
# Performing outlier treatment to all the variables
for(i in vars){
data_outlier <- outlier_treatment_fun(data_outlier,i)
}

# This a code that can be re-used after updating the column names in new_vars



## Import libraries
library(FactoMineR)
library(factoextra)
set.seed(123)

res.famd <- FAMD(data_outlier%>%
                        mutate(engineer = as.factor(engineer),
                               mba = as.factor(mba),
                               license = as.factor(license)), 
                 sup.var = 9,  ## Set the target variable "Churn" as a supplementary variable, so it is not included in the analysis for now
                 graph = FALSE, 
                 ncp=25)


pca_data <- get_famd_ind(res.famd)$coord


######correlation plot for variables, we choose 6 dimensions
corrplot::corrplot(get_famd_var(res.famd)$cos2, is.corr=FALSE)

data_use_outlier <- cbind.data.frame(pca_data, data%>%dplyr::select(Car))%>%
                            dplyr::select(Car, everything())%>%
                            clean_names()






```


```{r}


data_use_nb <- data_use_outlier%>%mutate(car = as.factor(if_else(car == "Car", 1,0)))%>%
                          mutate(ID = 1:nrow(data_use))

 


#####################################################

#set.seed(123)
set.seed(400)

train_no_car  <- data_use_nb%>%filter(car == 0)%>%
                                  group_by(car)%>%
                                  sample_n(267)


train_car <- data_use_nb%>%filter(car == 1)%>%
                        group_by(car)%>%
                        sample_n(24)




test_no_car <- data_use_nb %>%filter(car == 0)%>%
                              group_by(car)%>%
                              anti_join(train_no_car)



test_car <-   data_use_nb %>%filter(car == 1)%>%
                               group_by(car)%>%
                               anti_join(train_car)


train_set <- rbind.data.frame(train_car, train_no_car)
#####################randomise the order###############################
train_set <- train_set[sample(nrow(train_set)), 1:7]


test_set <- rbind.data.frame(test_car, test_no_car)
#####################randomise the order###############################
test_set <- test_set[sample(nrow(test_set)), 1:7]


knn_train <- train_set%>%dplyr::select(car, dim_1, dim_2,dim_3,dim_4, dim_5,dim_6 )%>%mutate(Label = as.factor(if_else( car== "1", "Car", "No_Car")))%>%
                         ungroup()%>%
                         dplyr::select(-car)%>%
                         mutate(Label = as.factor(as.character(Label)))

knn_test <- test_set%>%dplyr::select(car, dim_1, dim_2,dim_3,dim_4, dim_5,dim_6 )%>%mutate(Label = as.factor(if_else( car== "1", "Car", "No_Car")))%>%
                         ungroup()%>%
                         dplyr::select(-car)%>%
                         mutate(Label = as.factor(as.character(Label)))



```

#####capp8ing doesnt work
```{r}


set.seed(400)
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0:5,
  adjust = seq(0, 5, by = 1)
)


train_control <- trainControl(
  method="repeatedcv", 
  repeats = 10 ,
  classProbs=TRUE, 
  summaryFunction = twoClassSummary, 
  savePredictions=TRUE
  )

x_1 = setdiff(colnames(knn_train), c("Label"))

x <- as.data.frame(knn_train[,x_1])
y <- knn_train$Label

nb.m1 <- caret::train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control,
  tuneGrid = search_grid,
  preProc = c("BoxCox", "center", "scale")
  )


nb.m1$results %>% 
  top_n(5, wt = ROC) %>%
  arrange(desc(ROC))


NBpred <- predict(nb.m1, newdata = knn_test, type = "prob")
NBresponse <- predict(nb.m1, newdata = knn_test)
confusionMatrix(as.factor(NBresponse), as.factor(knn_test$Label))

#################

resample_stats <- thresholder(nb.m1, 
                              threshold = seq(.00, 1, by = 0.02), 
                              final = TRUE)

resample_stats%>%arrange(desc(Sensitivity))


####choose threshold of 
NB_predictions <- NBpred%>%mutate(final_class_0.04 = if_else(Car < 0.04, "No_Car", "Car"),
                                       final_class_0.08 = if_else(Car < 0.08, "No_Car", "Car"),
                                       final_class_0.10 = if_else(Car < 0.10, "No_Car", "Car"))


confusionMatrix( as.factor( NB_predictions$final_class_0.04),as.factor(knn_test$Label ))

plot(varImp(nb.m1, scale = T))

```

